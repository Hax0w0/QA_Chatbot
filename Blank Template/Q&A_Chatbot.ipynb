{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **[INSERT YOUR TOPIC] Q&A Chatbot**\n",
        "**Name**: [INSERT YOUR NAME]<br>\n",
        "**Organization**: Northwestern AI Club - Fall 2025<br>"
      ],
      "metadata": {
        "id": "xHN3ORrgAc_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Description**: For this project, you will build a chatbot that can answer questions about a specific topic of your choice (a sports team, favorite anime, etc.) by retrieving relevant information from the web and generating natural-language answers using a local LLM. This project is meant to serve as a very light introduction to some useful resources like Hugging Face and key concepts like retrieval-augmented generation (RAG), embeddings, cosine similarity, and LLM parameters like temperature. Unlike using a pre-packaged API, this project lets you work directly with models, giving you hands-on experience with prompt design, stopping conditions, and model behavior. By the end of this project, you should have a solid starting point to build more complex, topic-specific applications.\n",
        "\n",
        "> **Note**: This project gives a hands-on, practical taste of production LLM workflows (but MUCH less complex). The next project will be more focused on the architecture behind these models."
      ],
      "metadata": {
        "id": "vNNN1Z8GBCAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting Up Required Packages**"
      ],
      "metadata": {
        "id": "xomqTEyvV21A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Colab already comes with a bunch of useful packages installed, there are still a few packages that we have to manually install.<br>"
      ],
      "metadata": {
        "id": "Z6iQYYBIZwYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U ddgs\n",
        "!pip install -q -U newspaper3k\n",
        "!pip install -q -U --upgrade lxml_html_clean"
      ],
      "metadata": {
        "id": "I_5ShB8lWekx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can import all the libraries that we'll need for the project."
      ],
      "metadata": {
        "id": "zC8kTtO6aLQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import requests\n",
        "from ddgs import DDGS\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList"
      ],
      "metadata": {
        "id": "sgaWzcM5Vu74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Search Engine Setup**"
      ],
      "metadata": {
        "id": "2K4gO9IQ6uuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we'll be using DuckDuckGo as our search engine. When using DuckDuckGo's API, you'll notice it lacks many of the quality-of-life features that services like Google typically have like richer semantic understanding, advanced ranking algorithms, and personalized results. This means that we'll have to do a little more than just directly inputting our query and returning the first results.<br><br>\n",
        "\n",
        "For this part, you'll be tasked with refining your queries to produce more consistent results and identifying trusted domains that provide reliable information for your specific topic. Below are functions that have been provided for you to use. If you need any additional help understanding how they work, you can use resources like ChatGPT or ask during our next club meeting.\n",
        "\n",
        "> **Note**: If you want to add features to improve the search phase, please feel free! The code is quite modular, so you shouldn’t have to change much to customize it. You can personalize this project as much as you want :)"
      ],
      "metadata": {
        "id": "Q8u7CqrkMrpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sources(question, context_prefix, clarifiers, trusted_domains, num_sources_limit, display=False):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Given a question, find the most relevant sources to answer it.\n",
        "\n",
        "  Inputs:\n",
        "    * Question: The question that the user inputs.\n",
        "    * Context_Prefix: A prefix to put at the beginning of the query.\n",
        "    * Clarifiers: A dictionary of terms and their expansion.\n",
        "    * Trusted_Domains: A list of trusted domains to prioritize.\n",
        "    * Num_Sources_Limit: The maximum number of URLs to return.\n",
        "\n",
        "  Output:\n",
        "    * Best_Sources: A list of URLS to the most relevant webpages to answer the question.\n",
        "  \"\"\"\n",
        "  # Create the query using our context prefix & clarifiers\n",
        "  query = prepare_query(question, context_prefix, clarifiers)\n",
        "\n",
        "  # Get an initial list of results\n",
        "  source_urls = search_web(query, display)\n",
        "\n",
        "  # Filter the results to prioritize trusted domains\n",
        "  best_sources = filter_results(source_urls, trusted_domains, num_sources_limit)\n",
        "\n",
        "  # Show the results\n",
        "  if display:\n",
        "    print(f\"Final Results for \\\"{query}\\\":\")\n",
        "    for source in best_sources:\n",
        "      print(f\"\\t{source}\")\n",
        "\n",
        "  return best_sources\n",
        "\n",
        "def prepare_query(question, context_prefix, clarifiers):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Uses the question, context prefix, and clarifiers to create a query. Since DuckDuckGo's\n",
        "    API primarily uses pattern matching to retrieve results, this function adds a context\n",
        "    prefix and expands any clarifiers in the question to be more specific.\n",
        "\n",
        "  Inputs:\n",
        "    * Question: The question that the user inputs.\n",
        "    * Context_Prefix: A prefix to put at the beginning of the query.\n",
        "    * Clarifiers: A dictionary of terms and their expansion.\n",
        "\n",
        "  Output:\n",
        "    * Query: The query that will be used to search the web.\n",
        "  \"\"\"\n",
        "  for term, expansion in clarifiers.items():\n",
        "\n",
        "    # If the expansion is already present, don't expand the term\n",
        "    if expansion.lower() in question.lower():\n",
        "      continue\n",
        "\n",
        "    # Otherwise, replace the term in the question with the expansion\n",
        "    pattern = r'\\b' + re.escape(term) + r'\\b'\n",
        "    question = re.sub(pattern, expansion, question, flags=re.IGNORECASE)\n",
        "\n",
        "  # Add the context prefix\n",
        "  query = f\"{context_prefix} {question}\"\n",
        "\n",
        "  return query\n",
        "\n",
        "def search_web(query, display=False):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Takes in the query and uses DuckDuck go to search for results. Since DuckDuckGo's\n",
        "    API can be inconsistent, this function does 3 rounds of searches and aggregates\n",
        "    the top 5 results from each round.\n",
        "\n",
        "  Inputs:\n",
        "    * Query: The query that will be used to search the web.\n",
        "\n",
        "  Output:\n",
        "    * Source_Urls: A list of URLs for the aggregated results (top 7 from each round).\n",
        "  \"\"\"\n",
        "  source_urls = []\n",
        "\n",
        "  # Use DuckDuckGo as the search client\n",
        "  with DDGS() as search_client:\n",
        "\n",
        "    # Search 3 times and aggregate results\n",
        "    for round in range(3):\n",
        "      if display: print(\"Search Results For Round\", round+1)\n",
        "      results = search_client.text(query)\n",
        "\n",
        "      # Add the top 7 results from each search\n",
        "      for result in results[:7]:\n",
        "        source_url = result[\"href\"]\n",
        "        if display: print(f\"\\t{source_url}\")\n",
        "\n",
        "        # If it is a duplicate, don't add it\n",
        "        if source_url not in source_urls:\n",
        "          source_urls.append(source_url)\n",
        "\n",
        "      if display: print(\"\")\n",
        "\n",
        "  return source_urls\n",
        "\n",
        "def filter_results(source_urls, trusted_domains, num_sources_limit):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Filters the results to prioritize trusted domains. This function reduces\n",
        "    noisy and irrelevant results that DuckDuckGo's API can produce.\n",
        "\n",
        "  Inputs:\n",
        "    * Source_Urls: A list of URLs for the aggregated results.\n",
        "    * Trusted_Domains: A list of trusted domains to prioritize.\n",
        "    * Num_Sources_Limit: The maximum number of URLs to return.\n",
        "\n",
        "  Output:\n",
        "    * Best_Sources: A list of URLS to the most relevant webpages to answer the question.\n",
        "  \"\"\"\n",
        "  best_sources = []\n",
        "\n",
        "  # Add results that are from a trusted domain\n",
        "  for source in source_urls:\n",
        "\n",
        "    for domain in trusted_domains:\n",
        "      if domain in source:\n",
        "        best_sources.append(source)\n",
        "        break\n",
        "\n",
        "    if len(best_sources) == num_sources_limit:\n",
        "      break\n",
        "\n",
        "  # If we end up with less than the limit, add other sources\n",
        "  if len(best_sources) < num_sources_limit:\n",
        "    fallback_sources = [source for source in source_urls if source not in best_sources]\n",
        "    best_sources += fallback_sources[:num_sources_limit - len(best_sources)]\n",
        "\n",
        "  return best_sources"
      ],
      "metadata": {
        "id": "hfc3JiRczh-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As stated earlier, we're going to address the limitations of DuckDuckGo's API in 2 different ways. First, we're going to refine the query in order to get more consistent results. Second, we're going to filter the results to prioritize trusted domains. By having these 2 systems in place, we should be able to reduce irrelevant or noisy results that can reduce the quality of retrieved information or confuse the LLM.<br><br>\n",
        "\n",
        "Here is a breakdown of what you need to add:<br>\n",
        "\n",
        "> `Context Prefix`: DuckDuckGo's API primarily uses pattern matching to get results. By putting a context prefix at the beginning of the query, we can push the search engine toward more relevant results. For example, if your topic is about Northwestern, you can have something like [Northwestern University] as your context prefix.\n",
        "\n",
        "> `Clarifiers`: Pattern matching can be problematic if a term is ambiguous. For example, “Apple” could refer to the fruit or the company. To help clarify, you can expand ambiguous terms like \"Apple\" in your query to \"Apple fruit\" to be more specific.\n",
        "\n",
        "> `Trusted Domains`: Even though refining the query can produce more consistent results, DuckDuckGo can still produce irrelevant results. To make sure we get the best information, we can prioritize trusted domains and then look at other sources.\n"
      ],
      "metadata": {
        "id": "Bw8mrYeKVpGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\n",
        "\n",
        "context_prefix = \"\"\n",
        "\n",
        "clarifiers = {\"TERM\": \"EXPANSION\"}\n",
        "\n",
        "trusted_domains = [\"DOMAIN\"]\n",
        "\n",
        "num_sources_limit = _\n",
        "\n",
        "source_urls = find_sources(question, context_prefix, clarifiers, trusted_domains, num_sources_limit, display=True)"
      ],
      "metadata": {
        "id": "a1QMXtoZ4eja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping & Formatting Data**"
      ],
      "metadata": {
        "id": "7tOLvpRHNO_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a list of links to relevant webpages, we need to extract the content from those webpages to feed into our LLM. Scraping webpages can be quite annoying because websites can be structured differently. To help with this, there is code provided below for you to use. The code below attempts to scrape content using 2 different methods (Newspaper3k and requests).\n",
        "\n",
        "> **Note**: Scraping isn’t always perfect. Some websites (like Reddit) make scraping intentionally difficult and prefer that you use their official API. Don’t worry if some pages don’t work, the goal here is to get enough usable content to pass along to the LLM. You’re also welcome to modify the scraping code or try other libraries if you’re curious."
      ],
      "metadata": {
        "id": "7uUdHR69d6lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_webpages(source_urls, display=False):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Takes in the URLs for the best webpages and attempts to scrape the content\n",
        "    from those webpages to feed into the LLM.\n",
        "\n",
        "  Inputs:\n",
        "    * Source_URLs: A list of URLs for the best webpages.\n",
        "\n",
        "  Output:\n",
        "    * Webpages: A list of the scraped content from the webpages.\n",
        "  \"\"\"\n",
        "  webpages = []\n",
        "\n",
        "  for url in source_urls:\n",
        "\n",
        "    # First try using Newspaper3k to scrape the webpage\n",
        "    try:\n",
        "      webpage_text = scrape_webpage_newspaper3k(url)\n",
        "      webpages.append(webpage_text)\n",
        "\n",
        "      if display: print(f\"Successfully scraped {url} using newspaper3k.\\n\")\n",
        "\n",
        "    # If that fails, trying using Requests and BeautifulSoup\n",
        "    except:\n",
        "      if display: print(f\"Newspaper3k failed for {url}, falling back to requests.\\n\")\n",
        "      webpage_text = scrape_webpage_requests(url)\n",
        "\n",
        "      if webpage_text:\n",
        "        if display: print(f\"\\tSuccessfully scraped {url} using requests.\\n\")\n",
        "        webpages.append(webpage_text)\n",
        "\n",
        "      else:\n",
        "        if display: print(f\"\\tFailed to scrape {url} using both methods.\\n\")\n",
        "\n",
        "  return webpages\n",
        "\n",
        "def scrape_webpage_newspaper3k(url):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Uses Newspaper3k to scrape the content from the webpage.\n",
        "  \"\"\"\n",
        "  webpage = Article(url)\n",
        "  webpage.download()\n",
        "  webpage.parse()\n",
        "\n",
        "  return webpage.text\n",
        "\n",
        "def scrape_webpage_requests(url):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Uses Requests and BeautifulSoup to scrape the content from the webpage.\n",
        "  \"\"\"\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "\n",
        "  try:\n",
        "    # Make an HTTP GET request to the given URL\n",
        "    response = requests.get(url, headers=headers, timeout=10)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Identify paragraphs by looking for <p> tags\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "\n",
        "    # Join paragraphs together into one string separated by 2 newline characters\n",
        "    article_text = \"\\n\\n\".join(p.get_text() for p in paragraphs)\n",
        "\n",
        "    return article_text\n",
        "\n",
        "  except:\n",
        "    return None"
      ],
      "metadata": {
        "id": "-UROxGKLNfhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this section, you aren't required to code anything unless you want to modify the code. I would recommend skimming through some of the functions (especially `scrape_webpage_request`) to see how the code is scraping content incase you need to troubleshoot later.<br>\n",
        "\n",
        "> For now, run the code below to scrape the webpages we found during the search phase. If you notice that many pages are failing, you may need to tweak the scraping code or adjust your list of trusted domains."
      ],
      "metadata": {
        "id": "j-Ztwjp-qubX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "webpages = scrape_webpages(source_urls, display=True)"
      ],
      "metadata": {
        "id": "c3LYt3NIWH89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're going to load an embedding model to transform text into numerical vectors.\n",
        "> Make sure that your Hugging Face access token is labeled as `HF_TOKEN` in your notebook secrets."
      ],
      "metadata": {
        "id": "efwXONj9Q1ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sentence embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "-XdZXyFP5Q38",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the provided code for extracting the most relevant information from the webpages. This is done in 2 steps:\n",
        "\n",
        "> **Separate By Paragraphs**: First, we'll separate the content from each webpage by paragraph.\n",
        "\n",
        "> **Rank By Cosine Similarity**: Then, we'll calculate the cosine similarity between the question and each paragraph to determine which paragraphs are most relevant."
      ],
      "metadata": {
        "id": "mCy-hNukSygJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_best_paragraphs(question, webpages, max_num_paragraphs, display=False):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Given the question and the content on the webpages, extract the most relevant\n",
        "    paragraphs to answer the question.\n",
        "\n",
        "  Inputs:\n",
        "    * Question: The question that the user inputs.\n",
        "    * Webpages: A list of the scraped content from the webpages.\n",
        "    * Max_Num_Paragraphs: The maximum number of paragraphs to return.\n",
        "\n",
        "  Output:\n",
        "    * Best_Paragraphs: A list of the most relevant paragraphs to answer the question.\n",
        "  \"\"\"\n",
        "  # Seperate all the paragraphs in the webpages\n",
        "  paragraphs = split_webpages_into_paragraphs(webpages)\n",
        "\n",
        "  # Retrieve the best paragraphs\n",
        "  best_paragraphs = filter_paragraphs(question, paragraphs, max_num_paragraphs)\n",
        "\n",
        "  # Print results\n",
        "  if display:\n",
        "    print(\"Query:\", question, \"\\n\")\n",
        "    for paragraph in best_paragraphs:\n",
        "      print(f\"   {paragraph}\\n\")\n",
        "\n",
        "  return best_paragraphs\n",
        "\n",
        "def split_webpages_into_paragraphs(webpages):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Splits the content from each webpage by paragraph. We'll determine where\n",
        "    each paragraph starts by looking for newline characters.\n",
        "\n",
        "  Inputs:\n",
        "    * Webpages: A list of the scraped content from the webpages.\n",
        "\n",
        "  Output:\n",
        "    * Paragraphs: A list of all the paragraphs from the webpages provided.\n",
        "  \"\"\"\n",
        "  paragraphs = []\n",
        "\n",
        "  for webpage in webpages:\n",
        "\n",
        "      # Split by double newlines\n",
        "      raw_paragraphs = webpage.split(\"\\n\\n\")\n",
        "\n",
        "      # Clear whitespace characters from each paragraph\n",
        "      for p in raw_paragraphs:\n",
        "          cleaned = p.strip()\n",
        "          paragraphs.append(cleaned)\n",
        "\n",
        "  return paragraphs\n",
        "\n",
        "def filter_paragraphs(question, paragraphs, max_num_paragraphs):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Calculates the cosine similarity between the question and each paragraph\n",
        "    to determine which paragraphs are most relevant.\n",
        "\n",
        "  Inputs:\n",
        "    * Question: The question that the user inputs.\n",
        "    * Paragraphs: A list of all the paragraphs from the webpages provided.\n",
        "    * Max_Num_Paragraphs: The maximum number of paragraphs to return.\n",
        "\n",
        "  Output:\n",
        "    * Best_Paragraphs: A list of the most relevant paragraphs to answer the question.\n",
        "  \"\"\"\n",
        "  # Embed the question and each paragraph\n",
        "  question_embedding = embedding_model.encode([question])\n",
        "  paragraph_embeddings = embedding_model.encode(paragraphs)\n",
        "\n",
        "  # Compute cosine similarity between question and all paragraphs\n",
        "  similarities = cosine_similarity(question_embedding, paragraph_embeddings)[0]\n",
        "\n",
        "  # Pair paragraphs with similarity scores and sort in descending order\n",
        "  scored_paragraphs = list(zip(paragraphs, similarities))\n",
        "  scored_paragraphs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  # Select the best paragraphs\n",
        "  top_scores = scored_paragraphs[:max_num_paragraphs]\n",
        "  best_paragraphs = [paragraph for paragraph, score in top_scores]\n",
        "\n",
        "  return best_paragraphs"
      ],
      "metadata": {
        "id": "VXwBXbuyw3CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see what the content we've extracted looks like."
      ],
      "metadata": {
        "id": "_5j-Dx0f-mBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_paragraphs = extract_best_paragraphs(question, webpages, _, display=True)"
      ],
      "metadata": {
        "id": "xZRaIDuCzCZ6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuring Our LLM**"
      ],
      "metadata": {
        "id": "JFlBRNscaq-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load the model through Hugging Face. The default model that we'll be using is Qwen2.5 because it has been instruction-tuned for conversation tasks. You can also explore Hugging Face and use another model of your choice, just keep these few things in mind:\n",
        "\n",
        "> **1.** If you're using a model with a lot of parameters (more than 3 billion), consider using a bitsandbytes configuration to load the weights in 4-bit instead of 32-bit (only a few lines of code). This reduces memory usage and speeds up inference.\n",
        "\n",
        "> **2.** Not all models on Hugging Face are instruction-tuned, so performance may be worse if the model isn’t specifically trained for human dialogue. This can make a HUGE difference, so be sure to research the model."
      ],
      "metadata": {
        "id": "gUB0btQFfQRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the name of the model we want\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "# Load the model weights (if you use a bigger model, add quantization_config=bnb_config)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "# Get the specific tokenizer for this model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "atrulFIy-EHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to feed the question and the most relevant content from the webpages into the model. For the `tokenize_prompt` function, design the format of the prompt to feed into the model. The rest of the code in the function will tokenize the prompt for you."
      ],
      "metadata": {
        "id": "_QDUg6bI0h8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_prompt(question, paragraphs, tokenizer, max_input_tokens):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Given the question and the most relevant paragraphs, create and\n",
        "    tokenize the prompt to feed into the model.\n",
        "\n",
        "  Inputs:\n",
        "    * Question: The question that the user inputs.\n",
        "    * Paragraphs: A list of the most relevant paragraphs to answer the question.\n",
        "    * Tokenizer: The tokenizer for the model.\n",
        "    * Max_Input_Tokens: The maximum number of tokens allowed in the prompt.\n",
        "\n",
        "  Output:\n",
        "    * Prompt: The prompt to feed into the model.\n",
        "    * Tokenized_Prompt: The tokenized prompt\n",
        "  \"\"\"\n",
        "  # Create the prompt to feed into the model\n",
        "  combined_paragraphs = \"\\n\\n\".join(paragraphs)\n",
        "  prompt = (f\"DESIGN YOUR PROMPT HERE\")\n",
        "\n",
        "  # Tokenize the prompt\n",
        "  tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
        "  token_count = tokenized_prompt['input_ids'].shape[-1]\n",
        "\n",
        "  # Make sure the prompt isn't too long\n",
        "  if token_count > max_input_tokens:\n",
        "    raise ValueError(f\"Prompt is too long ({token_count} tokens), max allowed is {max_input_tokens}. Try decreasing number of paragraphs to include or increase the max input tokens.\")\n",
        "\n",
        "  return prompt, tokenized_prompt"
      ],
      "metadata": {
        "id": "glqt04_NlweV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with LLMs, it’s important to remember that we don’t just give them instructions and walk away. We also need to observe their behavior and add rules/guardrails for them to follow. Without moderation, the model may continue generating endlessly, start rambling, or hallucinate extra information.\n",
        "\n",
        "> While the `max_new_tokens` argument will prevent the model from generating infinitely, the model might ramble about random topics or constantly repeat itself until it hits that limit. By implementing custom stopping conditions, we can stop the model early to prevent that kind of behavior."
      ],
      "metadata": {
        "id": "jU-_1jBW41wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(tokenized_prompt, model, tokenizer, max_new_tokens, temperature):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Given the tokenized prompt, generate a response from the model.\n",
        "\n",
        "  Inputs:\n",
        "    * Tokenized_Prompt: The tokenized prompt.\n",
        "    * Model: The model to use.\n",
        "    * Tokenizer: The tokenizer for the model.\n",
        "    * Max_New_Tokens: The maximum number of tokens for the model to generate.\n",
        "    * Temperature: The temperature to use for sampling.\n",
        "\n",
        "  Output:\n",
        "    * Generated_Text: The generated text from the model.\n",
        "  \"\"\"\n",
        "  # Set up inputs by putting tensors onto the GPU\n",
        "  inputs = {key: tensor_value.to(model.device) for key, tensor_value in tokenized_prompt.items()}\n",
        "  prompt_length = inputs['input_ids'].shape[-1]\n",
        "\n",
        "  # Attach custom stopping criteria\n",
        "  stopping_criteria = StoppingCriteriaList([Custom_Stop_Conditions(tokenizer, prompt_length)])\n",
        "\n",
        "  outputs = model.generate(**inputs,\n",
        "                           temperature=temperature,\n",
        "                           max_new_tokens=max_new_tokens,\n",
        "                           do_sample=True,\n",
        "                           pad_token_id=tokenizer.eos_token_id,\n",
        "                           eos_token_id=tokenizer.eos_token_id,\n",
        "                           stopping_criteria=stopping_criteria)\n",
        "\n",
        "  generated_ids = outputs[0][prompt_length:]\n",
        "  generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "  return generated_text\n",
        "\n",
        "class Custom_Stop_Conditions(StoppingCriteria):\n",
        "  \"\"\"\n",
        "  Purpose:\n",
        "    Implements custom stopping logic for text generation. During generation,\n",
        "    this class is called repeatedly after each token is generated. It only looks\n",
        "    at tokens generated after the prompt and checks if the stopping conditions\n",
        "    have been met. If so, it signals the model to stop early.\n",
        "\n",
        "  Inputs (via __init__):\n",
        "    * Tokenizer: The tokenizer for the model (to decode the generated tokens).\n",
        "    * Prompt_Length: Number of tokens in the original prompt (to separate prompt vs. generated tokens).\n",
        "  \"\"\"\n",
        "  def __init__(self, tokenizer, prompt_length):\n",
        "    super().__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.prompt_length = prompt_length\n",
        "\n",
        "  def __call__(self, input_ids, scores, **kwargs):\n",
        "    # Decode only the newly generated tokens\n",
        "    generated_ids = input_ids[0][self.prompt_length:]\n",
        "    generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=False)\n",
        "\n",
        "    # Stop if <|endoftext|> token actually appears in the decoded text\n",
        "    if \"<|endoftext|>\" in generated_text:\n",
        "      return True\n",
        "\n",
        "    # Stop if EOS token appears\n",
        "    last_token = input_ids[0, -1].item()\n",
        "    if last_token == self.tokenizer.eos_token_id:\n",
        "      return True\n",
        "\n",
        "    # Stop if a newline character appears in the generated text\n",
        "    if \"\\n\" in generated_text:\n",
        "      return True\n",
        "\n",
        "    # Otherwise continue generating\n",
        "    return False"
      ],
      "metadata": {
        "id": "ofxJEsoE4tgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Product**"
      ],
      "metadata": {
        "id": "EnyxhKihI49f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_product(question, context_prefix, clarifiers, trusted_domains, num_sources_limit, max_num_paragraphs, model, tokenizer, max_input_tokens, max_new_tokens, temperature):\n",
        "\n",
        "  # Find the most relevant sources\n",
        "  source_urls = find_sources(question, context_prefix, clarifiers, trusted_domains, num_sources_limit)\n",
        "\n",
        "  # Scrape the webpages and extract the most relevant paragraphs\n",
        "  webpages = scrape_webpages(source_urls)\n",
        "  best_paragraphs = extract_best_paragraphs(question, webpages, max_num_paragraphs)\n",
        "\n",
        "  # Tokenize the prompt and feed it into the model\n",
        "  prompt, tokenized_prompt = tokenize_prompt(question, best_paragraphs, tokenizer, max_input_tokens)\n",
        "  generated_text = generate_response(tokenized_prompt, model, tokenizer, max_new_tokens, temperature)\n",
        "\n",
        "  return prompt, generated_text"
      ],
      "metadata": {
        "id": "TfJ9D-HUI39P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\n",
        "\n",
        "context_prefix = \"\"\n",
        "\n",
        "clarifiers = {\"TERM\": \"EXPANSION\"}\n",
        "\n",
        "trusted_domains = [\"DOMAIN\"]\n",
        "\n",
        "num_sources_limit = _\n",
        "\n",
        "max_num_paragraphs = _\n",
        "\n",
        "max_input_tokens = _\n",
        "\n",
        "max_new_tokens = _\n",
        "\n",
        "temperature = _\n",
        "\n",
        "prompt, generated_text = final_product(question, context_prefix, clarifiers, trusted_domains, num_sources_limit, max_num_paragraphs, model, tokenizer, max_input_tokens, max_new_tokens, temperature)\n",
        "\n",
        "print(f\"{prompt}\\n\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "XuQ-o24mPG2l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}